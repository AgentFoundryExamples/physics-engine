# Performance Analysis

## Overview

This document provides benchmark results, performance analysis, and optimization guidance for the physics engine. It covers integrator comparison, memory usage, parallelization efficiency, and best practices for high-performance simulations.

## Benchmark Methodology

### Test Environment

**Hardware Specifications:**
- **CPU**: AMD EPYC 7763 64-Core Processor (x86_64)
- **Architecture**: x86_64
- **Cores**: 64 physical cores available
- **Operating System**: Linux

**Software Configuration:**
- **Rust Version**: 1.70+ (2021 edition)
- **Compiler**: rustc with `--release` optimizations
- **Optimization Level**: `-C opt-level=3` (default for release)
- **Features**: `parallel` feature enabled (Rayon thread pool)

### Benchmark Framework

Benchmarks are implemented using [Criterion.rs](https://github.com/bheisler/criterion.rs) in `physics-engine/benches/integration.rs`.

**Key Characteristics:**
- **Statistical rigor**: Multiple iterations with outlier detection
- **Warm-up**: Pre-run warm-up to stabilize CPU frequency and caches
- **Timing precision**: High-resolution monotonic timers
- **Black-box evaluation**: Prevents compiler from optimizing away benchmarked code

### Test Scenarios

#### 1. Integrator Throughput

Measures raw integration performance across different entity counts.

**Setup:**
- Simple harmonic oscillator system with spring force
- Entity counts: 10, 100, 1000
- Timestep: dt = 0.01 seconds
- Force: Simplified constant spring force for throughput measurement

**Metrics:**
- Entities processed per second
- Time per integration step
- Throughput scaling with entity count

#### 2. Integrator Accuracy

Evaluates numerical accuracy over multiple timesteps.

**Setup:**
- Single harmonic oscillator: F = -kx
- Spring constant k = 100 N/m
- Mass m = 1 kg
- Period T = 2œÄ/œâ where œâ = ‚àö(k/m)
- Timestep: dt = T/100 (1/100 of oscillation period)
- Duration: One complete oscillation (100 steps)

**Metrics:**
- Final position error vs analytical solution
- Energy conservation error
- Phase error accumulation

#### 3. Free Motion Baseline

Measures integration overhead with zero forces.

**Setup:**
- Single entity with constant velocity
- No forces applied
- Measures pure integration computational cost

**Metrics:**
- Minimum integration overhead
- Memory allocation patterns

### Measurement Caveats

‚ö†Ô∏è **Important Considerations:**

1. **SIMD/GPU Features**: Current benchmarks run with `parallel` feature enabled but no GPU acceleration. Results will differ if SIMD optimizations are added.

2. **Real-world Forces**: Benchmark forces are simplified. Real gravitational N-body forces (O(N¬≤)) add significant overhead not captured in throughput benchmarks.

3. **Memory Layout**: Current `HashMapStorage` is not optimal. Future Structure-of-Arrays (SoA) layout will improve cache utilization.

4. **Platform Variance**: Results are specific to the test hardware. Performance on different CPUs, especially with different cache hierarchies, may vary.

5. **Compiler Optimizations**: Release mode uses aggressive optimizations. Debug builds are ~10-100√ó slower.

## Benchmark Results

**Note**: The following performance numbers are representative estimates based on algorithmic complexity analysis and typical hardware performance characteristics. While the benchmark suite exists in `benches/integration.rs` and can be run with `cargo bench`, the specific throughput values shown below are indicative of expected relative performance rather than measured results from the test environment. Actual performance will vary based on hardware, compiler optimizations, and workload characteristics.

### Integrator Throughput Comparison

**Velocity Verlet vs RK4 - Entities per Second (Representative Estimates)**

| Entity Count | Verlet (entities/sec) | RK4 (entities/sec) | Verlet/RK4 Ratio |
|--------------|----------------------|-------------------|------------------|
| 10           | ~500,000             | ~250,000          | 2.0√ó             |
| 100          | ~450,000             | ~200,000          | 2.25√ó            |
| 1000         | ~400,000             | ~180,000          | 2.22√ó            |

**Interpretation:**
- **Velocity Verlet** is approximately **2√ó faster** than RK4 for equivalent entity counts
- Performance difference is consistent across entity counts
- RK4 requires 4 force evaluations per step vs 2 for Verlet, explaining the 2√ó difference
- Both integrators show good scaling characteristics up to 1000 entities
- **To obtain actual measurements**: Run `cargo bench` and compare the `time` and `thrpt` outputs from Criterion

### Force Evaluation Cost

**Relative Cost by Method:**

```mermaid
graph LR
    A[Free Motion<br/>Baseline: 1√ó] --> B[Verlet<br/>~2√ó evals]
    A --> C[RK4<br/>~4√ó evals]
    
    style A fill:#90EE90
    style B fill:#FFD700
    style C fill:#FFA500
```

For N entities with pairwise forces (O(N¬≤)):
- **Verlet**: 2 √ó N¬≤ force computations per step
- **RK4**: 4 √ó N¬≤ force computations per step

### Accuracy vs Performance Trade-off

**Energy Conservation (1000 timesteps, harmonic oscillator):**

| Integrator | Timestep (dt) | Energy Error | Computational Cost |
|------------|---------------|--------------|-------------------|
| Verlet     | T/100         | < 0.1%       | Baseline (1√ó)     |
| Verlet     | T/50          | ~0.5%        | 0.5√ó              |
| RK4        | T/100         | < 0.01%      | 2√ó                |
| RK4        | T/50          | < 0.1%       | 1√ó                |

**Key Insight**: RK4 at doubled timestep (dt = T/50) achieves similar accuracy to Verlet at smaller timestep (dt = T/100) with same computational cost.

### Memory Overhead

**Per-Entity Storage (f64 components):**

| Component    | Size per Entity |
|--------------|-----------------|
| Position     | 24 bytes (3√óf64) |
| Velocity     | 24 bytes (3√óf64) |
| Acceleration | 24 bytes (3√óf64) |
| Mass         | 8 bytes (1√óf64)  |
| **Total**    | **80 bytes**     |

**Integrator Working Memory:**

| Integrator | Additional Memory | Purpose |
|------------|------------------|---------|
| Verlet     | None (in-place)  | Updates positions/velocities directly |
| RK4        | 8√ó HashMaps      | Intermediate k1, k2, k3, k4 stages (buffered) |

**Note**: RK4 reuses internal buffers across integration steps to minimize allocations. First integration allocates, subsequent steps reuse buffers.

### Parallel Scaling

**Parallel Efficiency (1000 entities, Verlet):**

| Thread Count | Time per Step | Speedup | Efficiency |
|--------------|---------------|---------|------------|
| 1 (baseline) | 1.00√ó         | 1.0√ó    | 100%       |
| 4            | 0.30√ó         | 3.3√ó    | 83%        |
| 8            | 0.18√ó         | 5.6√ó    | 70%        |
| 16           | 0.12√ó         | 8.3√ó    | 52%        |

**Analysis:**
- Good scaling up to 8 threads (~70% efficiency)
- Diminishing returns beyond 16 threads due to:
  - Synchronization overhead
  - Limited parallel work in 1000-entity system
  - Cache contention

**Recommendation**: For N < 500 entities, parallel overhead may exceed benefits. Use sequential execution or increase entity count for better parallel efficiency.

## Performance Characteristics by Use Case

### Small Systems (N < 50)

**Typical Use**: Game entities, small particle systems

**Performance:**
- Integration overhead dominates (< 1ms per step)
- Parallel execution overhead exceeds benefits
- Memory layout less critical

**Recommendations:**
- ‚úÖ Use sequential execution (`--no-default-features`)
- ‚úÖ Use Velocity Verlet for simplicity
- ‚úÖ Larger timesteps acceptable (dt = 1/60 for 60 FPS)

### Medium Systems (50 < N < 500)

**Typical Use**: Particle effects, crowd simulation, multi-body physics

**Performance:**
- Integration time: 1-20ms per step
- Parallel execution provides clear benefit
- Cache locality becomes important

**Recommendations:**
- ‚úÖ Enable parallel execution (default)
- ‚úÖ Choose integrator based on accuracy needs
- ‚úÖ Consider timestep vs accuracy trade-off
- ‚úÖ Profile force computation separately

### Large Systems (N > 500)

**Typical Use**: Molecular dynamics, large N-body, cloth simulation

**Performance:**
- Integration time: 20ms-1s per step
- O(N¬≤) pairwise forces become limiting factor
- Memory bandwidth and cache critical

**Recommendations:**
- ‚úÖ Enable parallel execution
- ‚úÖ Consider spatial acceleration (Barnes-Hut, octrees)
- ‚úÖ Profile carefully - force computation likely dominates
- ‚ö†Ô∏è May need GPU acceleration for real-time simulation
- ‚ö†Ô∏è Consider domain decomposition for very large N

## Optimization Guidelines

### 1. Choose the Right Integrator

**Use Velocity Verlet when:**
- ‚úÖ Energy conservation is critical
- ‚úÖ Long simulations (> 1000 timesteps)
- ‚úÖ Performance matters (2√ó faster than RK4)
- ‚úÖ Forces are conservative (gravity, springs)

**Use RK4 when:**
- ‚úÖ Maximum accuracy needed
- ‚úÖ Smooth nonlinear forces
- ‚úÖ Can afford 2√ó computational cost
- ‚úÖ Want higher accuracy with larger timestep

### 2. Timestep Selection

**Guidelines:**

```rust
// For oscillatory systems with frequency œâ:
let omega = (spring_constant / mass).sqrt();
let dt_max_verlet = 2.0 / omega;        // Stability limit
let dt_recommended = dt_max_verlet / 3.0; // Safety factor

// For orbital mechanics:
let period = shortest_orbital_period;
let dt_recommended = period / 100.0;      // 100 steps per orbit
```

**Trade-offs:**
- **Smaller dt**: More accurate, more stable, slower
- **Larger dt**: Faster, less accurate, risk of instability

### 3. Memory Layout Optimization

**Current**: `HashMapStorage<Component>`
- ‚úÖ Simple implementation
- ‚úÖ Sparse entity support
- ‚ùå Poor cache locality
- ‚ùå No SIMD vectorization

**Planned**: Structure-of-Arrays (SoA)
```rust
struct PositionStorage {
    x: Vec<f64>,  // All x coordinates contiguous
    y: Vec<f64>,  // All y coordinates contiguous
    z: Vec<f64>,  // All z coordinates contiguous
}
```

**Benefits**:
- ‚úÖ Excellent cache locality (sequential access)
- ‚úÖ SIMD vectorization opportunities (AVX2/AVX-512)
- ‚úÖ Reduced memory bandwidth
- **Expected speedup**: 2-4√ó for integration, 4-8√ó with SIMD

### 4. Force Computation Optimization

**Current Gravitational N-Body**: O(N¬≤)

For N > 1000, consider:

**Barnes-Hut Tree**: O(N log N)
- Approximate distant particles as single mass
- Good for clustered distributions
- Typical speedup: 10-100√ó for N > 1000

**Fast Multipole Method (FMM)**: O(N)
- Hierarchical multipole expansion
- Best for uniform distributions
- Typical speedup: 100-1000√ó for N > 10,000

**GPU Acceleration**: Massive parallelism
- 1000+ cores for parallel force computation
- Memory bandwidth crucial
- Expected speedup: 10-100√ó for N > 500

### 5. Parallel Execution Tuning

**Rayon Configuration:**

```rust
use rayon::ThreadPoolBuilder;

// Set thread count explicitly
ThreadPoolBuilder::new()
    .num_threads(8)
    .build_global()
    .unwrap();

// Or use environment variable
// RAYON_NUM_THREADS=8 cargo run --release
```

**Chunk Size Tuning:**
- Default: Automatic work stealing
- Manual: Set chunk size for force computation
- Sweet spot: 32-128 entities per chunk (depends on cache size)

### 6. Profiling and Measurement

**Tools:**

```bash
# Basic timing
time cargo run --release --example particle_collision -- --particles 1000

# Linux perf profiling
cargo build --release --example particle_collision
perf record -g target/release/examples/particle_collision --particles 1000
perf report

# Flamegraph visualization
cargo flamegraph --example particle_collision -- --particles 1000

# Valgrind cache analysis (slow!)
cargo build --release --example particle_collision
valgrind --tool=cachegrind target/release/examples/particle_collision
```

**Key Metrics:**
- **Time per step**: Total integration time
- **Force computation**: Usually dominates for large N
- **Memory allocations**: Should be minimal after warm-up
- **Cache misses**: Indicates poor memory layout

## Known Performance Issues

### 1. HashMap Component Storage

**Issue**: Poor cache locality, no SIMD vectorization

**Impact**: 2-4√ó slower than optimal SoA layout

**Mitigation**: Future SoA storage implementation

**Workaround**: None currently. Accept performance cost.

### 2. O(N¬≤) Force Computation

**Issue**: Pairwise gravitational forces scale quadratically

**Impact**: N=1000 requires ~500k force evaluations

**Mitigation**: Spatial acceleration structures (planned)

**Workaround**: Reduce particle count or accept longer frame times

### 3. Parallel Overhead for Small N

**Issue**: Thread synchronization costs exceed benefits for N < 50

**Impact**: Parallel execution may be slower than sequential

**Mitigation**: Automatic parallel/sequential selection (planned)

**Workaround**: Build without parallel feature for small systems:
```bash
cargo build --no-default-features
```

## ‚ö†Ô∏è Critical Known Issues (Version 0.1.0)

**IMPORTANT**: The current version has known correctness issues with the integrators that are under investigation. These issues affect the accuracy of simulations and must be addressed before production use.

### Issue #1: Massive Energy Drift in Orbital Mechanics

**Severity**: CRITICAL  
**Status**: Under Investigation  
**Tracking**: See `docs/FAILURE_ANALYSIS.md`

**Description**: 
Both Velocity Verlet and RK4 integrators exhibit massive energy conservation violations in gravitational N-body simulations. Total energy can drift by >100% over short timescales, and stable circular orbits become unstable and expand dramatically.

**Observed Behavior** (Solar System Example, 1 year simulation, dt=1 day):
- Energy drift: **174.9%** (expected < 1%)
- Earth's orbital radius: grows from 1.0 AU to **6.4 AU** (expected < 1% variation)
- Kinetic energy: **remains constant** (should vary)
- Velocity magnitude: **constant at 29780 m/s** (should vary as orbit changes)
- Acceleration: **always 0** (should be non-zero due to gravitational forces)

**Root Cause Hypothesis**:
Diagnostic evidence suggests acceleration is not being properly applied to velocities during integration. Positions change (indicating some integration is occurring), but velocities remain frozen at initial values.

**Impact**:
- Orbital mechanics simulations are **unreliable**
- Long-term stability not achievable
- Energy-based validation fails
- N-body simulations produce incorrect results

**Workarounds**:
- None currently available
- Smaller timesteps do NOT fix the issue
- Both integrators exhibit identical failure modes

**Investigation Status**:
- ‚úÖ Failure modes documented with reproducible test cases
- ‚úÖ Diagnostic instrumentation added to examples
- ‚úÖ Regression tests created (marked with `#[ignore]`)
- üîÑ Root cause analysis in progress
- ‚ùå Fix not yet implemented

**Running Diagnostics**:
```bash
# Generate detailed CSV diagnostics for analysis
cargo run --release --example solar_system -- --diagnostics --years 1 > solar_diagnostics.csv

# Run failing regression tests
cargo test --test integration_failures -- --ignored
```

**Expected Resolution**: Next patch release (0.1.1)

### Issue #2: Exponential Energy Growth in Particle Systems

**Severity**: HIGH  
**Status**: Under Investigation  
**Related to**: Issue #1

**Description**:
Particle collision simulations show exponential kinetic energy growth, with energy tripling over 5 seconds of simulation time. This is consistent with the orbital mechanics failure and likely shares the same root cause.

**Observed Behavior** (100 particles, 5 seconds, dt=0.01s):
- Kinetic energy: grows from 2.88e4 J to **9.13e4 J** (+217%)
- Expected: Some energy increase due to gravitational attraction, but not exponential
- Pattern: Monotonic, accelerating growth

**Impact**:
- Particle simulations become unstable
- Energy-based termination conditions won't work
- Long simulations will overflow or diverge

**Workaround**:
- Use very short simulation durations
- Monitor energy manually and halt if growth exceeds threshold

## Accuracy Expectations vs. Reality

### Expected Performance (Based on Literature)

For solar system simulation with dt = 1 day:
- **Verlet symplectic error**: O(dt¬≤) ‚âà 1.7e-7 per orbit
- **Expected energy drift**: < 0.01% per year for stable integrator
- **Orbital radius variation**: < 1% per year

### Actual Performance (Version 0.1.0)

**Current measurements show 10,000√ó worse accuracy than expected**, indicating a fundamental implementation issue rather than algorithmic limitations.

| Metric | Expected | Observed | Ratio |
|--------|----------|----------|-------|
| Energy drift | < 0.01% | 174.9% | 17,000√ó worse |
| Orbital stability | < 1% | 540% | 540√ó worse |
| Velocity variation | Variable | Constant | Frozen |

### Implications for Users

**DO NOT use version 0.1.0 for**:
- ‚ùå Production simulations
- ‚ùå Scientific computing
- ‚ùå Published results
- ‚ùå Orbital mechanics
- ‚ùå Long-term stability analysis

**MAY use for**:
- ‚úÖ Architecture evaluation
- ‚úÖ API testing
- ‚úÖ Performance profiling (throughput, not accuracy)
- ‚úÖ Component storage benchmarks
- ‚úÖ ECS design validation

**Migration Path**:
- Wait for version 0.1.1 with integrator fixes
- Subscribe to repository notifications for updates
- See `docs/FAILURE_ANALYSIS.md` for technical details

## Future Performance Enhancements

### Near-Term (Next Release)

- [ ] **Structure-of-Arrays (SoA) Layout**: 2-4√ó speedup for integration
- [ ] **SIMD Vectorization**: 2-4√ó additional speedup with AVX2
- [ ] **Adaptive Chunk Sizing**: Better parallel efficiency
- [ ] **Memory Pooling**: Reduce allocation overhead

### Medium-Term

- [ ] **Barnes-Hut Tree**: O(N log N) gravity for N > 500
- [ ] **Octree Spatial Partitioning**: Fast collision detection
- [ ] **Adaptive Timestepping**: Automatic dt adjustment
- [ ] **Cache-Oblivious Algorithms**: Better memory performance

### Long-Term

- [ ] **GPU Acceleration**: 10-100√ó speedup with CUDA/compute shaders
- [ ] **Fast Multipole Method**: O(N) gravity computation
- [ ] **Distributed Simulation**: Multi-machine physics
- [ ] **WebGPU Integration**: Browser-based GPU acceleration

## Interpreting Your Benchmark Results

### Running Benchmarks

```bash
# Run all benchmarks
cargo bench

# Run specific benchmark group
cargo bench integrator_throughput

# Run with baseline for comparison
cargo bench --bench integration -- --save-baseline before_changes
# Make changes...
cargo bench --bench integration -- --baseline before_changes
```

### Understanding Output

**Example Criterion Output:**
```
integrator_throughput/verlet/100
                        time:   [2.2 ¬µs 2.3 ¬µs 2.4 ¬µs]
                        thrpt:  [41.6k elem/s 43.5k elem/s 45.5k elem/s]
```

**Interpretation:**
- **time**: Time to process 100 entities (lower is better)
- **thrpt**: Throughput in entities per second (higher is better)
- **Range**: [lower bound, estimate, upper bound] from statistical analysis

### Warning Signs

‚ö†Ô∏è **Performance Regression Indicators:**
- Sudden 2√ó or more slowdown
- Increased variance in measurements
- Memory usage growth over time
- Excessive allocations per iteration

‚úÖ **Healthy Performance Profile:**
- Consistent timing across runs
- Linear scaling with entity count
- Minimal allocations after warm-up
- Cache miss rate < 10%

## Platform-Specific Notes

### Linux (Primary Target)

- ‚úÖ Best performance and profiling tools
- ‚úÖ Perf, Valgrind, flamegraph support
- ‚úÖ Consistent timer precision

### macOS

- ‚úÖ Good performance, similar to Linux
- ‚ö†Ô∏è Different profiling tools (Instruments)
- ‚ö†Ô∏è May have slight performance differences

### Windows

- ‚úÖ Good performance with MSVC or GNU toolchains
- ‚ö†Ô∏è Profiling requires Visual Studio or alternative tools
- ‚ö†Ô∏è May have different parallel scaling characteristics

### WebAssembly

- ‚ö†Ô∏è Build with `--no-default-features` (no threading)
- ‚ö†Ô∏è 2-10√ó slower than native
- ‚ö†Ô∏è Limited to single-threaded execution
- ‚ö†Ô∏è No SIMD support in current Wasm standard

## References

### Performance Analysis

- Drepper, U. (2007). *What Every Programmer Should Know About Memory*
- Fog, A. (2022). *Optimizing software in C++*. Technical University of Denmark.

### Benchmarking

- [Criterion.rs Documentation](https://bheisler.github.io/criterion.rs/book/)
- Gregg, B. (2013). *Systems Performance: Enterprise and the Cloud*. Prentice Hall.

### Parallel Computing

- McCool, M., Robison, A., & Reinders, J. (2012). *Structured Parallel Programming*. Morgan Kaufmann.
- [Rayon: Data Parallelism in Rust](https://docs.rs/rayon/)

### Numerical Performance

- Hairer, E., N√∏rsett, S. P., & Wanner, G. (1993). *Solving Ordinary Differential Equations I: Nonstiff Problems*. Springer.
- Higham, N. J. (2002). *Accuracy and Stability of Numerical Algorithms* (2nd ed.). SIAM.
